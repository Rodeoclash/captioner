import * as React from "react";
import type { NextPage } from "next";
import Head from "next/head";
import styles from "../styles/Home.module.css";
import { createModel } from "vosk-browser";

interface VoskResult {
  result: Array<{
    conf: number;
    start: number;
    end: number;
    word: string;
  }>;
  text: string;
}

const Home: NextPage = () => {
  const [utterances, setUtterances] = React.useState<VoskResult[]>([]);
  const videoRef = React.useRef<HTMLVideoElement | null>(null);

  const handleMetaDataLoaded = React.useCallback(async () => {
    const videoEl = videoRef.current;

    if (videoEl === null) {
      return;
    }

    var ctx = new AudioContext();
    // create an source node from the <video>
    var audioSource = ctx.createMediaElementSource(videoEl);
    // now a MediaStream destination node
    var stream_dest = ctx.createMediaStreamDestination();
    // connect the source to the MediaStream
    audioSource.connect(stream_dest);

    const channel = new MessageChannel();

    const model = await createModel(
      "/models/" + "vosk-model-small-ja-0.22.zip"
    );
    model.registerPort(channel.port1);

    const recognizer = new model.KaldiRecognizer(48000);
    recognizer.setWords(true);

    recognizer.on("result", (message: any) => {
      const result: VoskResult = message.result;

      if (result.text === "") {
        return;
      }

      setUtterances((utt: VoskResult[]) => [...utt, result]);
    });

    const audioContext = new AudioContext();
    await audioContext.audioWorklet.addModule("recognizer-processor.js");
    const recognizerProcessor = new AudioWorkletNode(
      audioContext,
      "recognizer-processor",
      { channelCount: 1, numberOfInputs: 1, numberOfOutputs: 1 }
    );
    recognizerProcessor.port.postMessage(
      { action: "init", recognizerId: recognizer.id },
      [channel.port2]
    );
    recognizerProcessor.connect(audioContext.destination);

    const source = audioContext.createMediaStreamSource(stream_dest.stream);
    source.connect(recognizerProcessor);

    videoEl.play();
  }, [videoRef]);

  const handleChange = React.useCallback(
    (e) => {
      const videoEl = videoRef.current;

      if (videoEl === null) {
        return;
      }

      videoEl.addEventListener("loadedmetadata", handleMetaDataLoaded);

      const file = e.target.files[0];
      const url = URL.createObjectURL(file);
      videoEl.src = url;
    },
    [videoRef]
  );

  const renderedUtterances = utterances.map((utterance) => {
    const first = utterance.result[0];
    return (
      <li key={first.start}>
        <p>
          [{first.start}]: {utterance.text}
        </p>
      </li>
    );
  });

  return (
    <div className={styles.container}>
      <Head>
        <title>Create Next App</title>
        <meta name="description" content="Generated by create next app" />
        <link rel="icon" href="/favicon.ico" />
      </Head>
      <div>
        <input type="file" onChange={(e) => handleChange(e)} />
      </div>
      <div style={{ display: "flex" }}>
        <video ref={videoRef} controls={true} />
        <div>
          <ol>{renderedUtterances}</ol>
        </div>
      </div>
    </div>
  );
};

export default Home;
